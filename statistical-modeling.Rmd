---
title: "Statistical Modeling Project"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    theme: flatly
    highlight: zenburn
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 6)
library(tidyverse)
library(broom)
library(car)
library(caret)
library(performance)
```

## Project Overview

### Research Question

State your research question or hypothesis clearly.

### Methodology

Describe the statistical methods you will use:

- Model type (e.g., linear regression, logistic regression, time series)
- Variables (dependent and independent)
- Assumptions to be tested
- Validation approach

## Data Preparation

```{r data-prep}
# Load and prepare data
data <- mtcars

# Check for missing values
missing_summary <- data %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count")

knitr::kable(missing_summary, caption = "Missing Values Summary")
```

### Data Splitting

```{r split-data}
set.seed(123)
training_index <- createDataPartition(data$mpg, p = 0.8, list = FALSE)
train_data <- data[training_index, ]
test_data <- data[-training_index, ]

cat("Training set size:", nrow(train_data), "
")
cat("Test set size:", nrow(test_data), "
")
```

## Model Building

### Model 1: Simple Linear Regression

```{r model1}
model1 <- lm(mpg ~ wt, data = train_data)
tidy(model1) %>% knitr::kable(digits = 4)
glance(model1) %>% knitr::kable(digits = 4)
```

### Model 2: Multiple Linear Regression

```{r model2}
model2 <- lm(mpg ~ wt + hp + cyl + am, data = train_data)
tidy(model2) %>% knitr::kable(digits = 4)
glance(model2) %>% knitr::kable(digits = 4)
```

### Model 3: Polynomial Regression

```{r model3}
model3 <- lm(mpg ~ poly(wt, 2) + hp + cyl + am, data = train_data)
tidy(model3) %>% knitr::kable(digits = 4)
glance(model3) %>% knitr::kable(digits = 4)
```

## Model Diagnostics

### Residual Analysis

```{r residuals}
par(mfrow = c(2, 2))
plot(model2)
```

### Multicollinearity Check

```{r vif}
vif_values <- vif(model2)
knitr::kable(data.frame(Variable = names(vif_values), VIF = vif_values),
             row.names = FALSE, digits = 2)
```

### Normality Tests

```{r normality}
shapiro_test <- shapiro.test(residuals(model2))
cat("Shapiro-Wilk Test p-value:", shapiro_test$p.value, "
")
```

## Model Comparison

```{r model-comparison}
compare_performance(model1, model2, model3) %>%
  knitr::kable(digits = 4)
```

### ANOVA Comparison

```{r anova}
anova(model1, model2, model3) %>% knitr::kable(digits = 4)
```

## Model Validation

### Predictions on Test Set

```{r predictions}
predictions <- predict(model2, newdata = test_data)
results <- data.frame(
  Actual = test_data$mpg,
  Predicted = predictions,
  Residual = test_data$mpg - predictions
)

knitr::kable(head(results, 10), digits = 2)
```

### Performance Metrics

```{r metrics}
rmse <- sqrt(mean((results$Actual - results$Predicted)^2))
mae <- mean(abs(results$Actual - results$Predicted))
r_squared <- cor(results$Actual, results$Predicted)^2

metrics <- data.frame(
  Metric = c("RMSE", "MAE", "R-squared"),
  Value = c(rmse, mae, r_squared)
)

knitr::kable(metrics, digits = 4)
```

### Actual vs Predicted Plot

```{r pred-plot}
ggplot(results, aes(x = Actual, y = Predicted)) +
  geom_point(color = "steelblue", size = 3) +
  geom_abline(intercept = 0, slope = 1, color = "coral", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Actual vs Predicted Values",
       x = "Actual MPG", y = "Predicted MPG")
```

## Conclusions

### Key Findings

Summarize your key findings:

1. Best performing model
2. Significant predictors
3. Model assumptions validity
4. Prediction accuracy

### Recommendations

Provide recommendations based on your analysis.

### Limitations

Discuss any limitations or caveats.

## References

List any references or data sources used.
